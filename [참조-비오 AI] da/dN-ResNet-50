import os
import glob
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, BatchNormalization, Add, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.mixed_precision import set_global_policy
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import cv2
from scipy.io import loadmat
set_global_policy('mixed_float16')

try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.TPUStrategy(tpu)
except ValueError:
    strategy = tf.distribute.get_strategy()

print("REPLICAS: ", strategy.num_replicas_in_sync)

TRAIN_PATH_1 = 'your_folder_path'
TRAIN_PATH_2 = 'your_folder_path'
TEST_PATH = 'your_folder_path'
EXCEL_FILE_PATH = 'your_folder_path'
WEIGHTS_SAVE_PATH = 'your_folder_path'

excel_data = pd.read_excel(EXCEL_FILE_PATH)
dadN_mapping = {row['image']: row['dadN'] for _, row in excel_data.iterrows()}

def load_mat_file(file_path, key):
    mat_data = loadmat(file_path)
    if key in mat_data:
        return mat_data[key]
    else:
        raise KeyError(f"'{key}' 키가 MATLAB 파일에 존재하지 않습니다. 사용 가능한 키: {list(mat_data.keys())}")

def load_png_file(file_path, target_shape=(128, 128)):  
    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)
    img_resized = cv2.resize(img, target_shape)
    return (img_resized / 127.5) - 1  

def z_score_normalize(data):
    mean = np.mean(data)
    std = np.std(data)
    return (data - mean) / (std + 1e-7)  

def load_displacement_data_paths(base_path, dadN_mapping):
    data_paths = []
    dadN_values = []
    
    for folder_name in os.listdir(base_path):
        folder_path = os.path.join(base_path, folder_name)
        if os.path.isdir(folder_path) and folder_name in dadN_mapping:
            u_files = glob.glob(os.path.join(folder_path, 'deltau', '*.mat'))
            v_files = glob.glob(os.path.join(folder_path, 'deltav', '*.mat'))
            ml_files = glob.glob(os.path.join(folder_path, 'ml0', '*.png'))
            
            for u_file, v_file, ml_file in zip(u_files, v_files, ml_files):
                data_paths.append((u_file, v_file, ml_file))
                dadN_values.append(dadN_mapping[folder_name])
    
    return data_paths, np.array(dadN_values)

def preprocess_data(data_paths, target_shape=(128, 128)):
    data = []
    for u_path, v_path, ml_path in data_paths:
        u_data = load_mat_file(u_path, 'subtracted_u')
        u_data_resized = cv2.resize(u_data, target_shape)
        u_data_normalized = z_score_normalize(u_data_resized)          
        v_data = load_mat_file(v_path, 'subtracted_v')
        v_data_resized = cv2.resize(v_data, target_shape)
        v_data_normalized = z_score_normalize(v_data_resized)        
        ml_data = load_png_file(ml_path, target_shape)                
        combined_data = np.stack([u_data_normalized, v_data_normalized, ml_data], axis=-1)
        data.append(combined_data)
    return np.array(data)

train_paths_1, dadN_values_1 = load_displacement_data_paths(TRAIN_PATH_1, dadN_mapping)
train_paths_2, dadN_values_2 = load_displacement_data_paths(TRAIN_PATH_2, dadN_mapping)
test_paths, dadN_values_test = load_displacement_data_paths(TEST_PATH, dadN_mapping)

all_train_paths = train_paths_1 + train_paths_2
all_dadN_values = np.concatenate([dadN_values_1, dadN_values_2])

x_train = preprocess_data(all_train_paths)
y_train = all_dadN_values

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)
scaler = StandardScaler()
y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
y_val_scaled = scaler.transform(y_val.reshape(-1, 1)).flatten()
y_test_scaled = scaler.transform(dadN_values_test.reshape(-1, 1)).flatten()

def residual_block(x, filters, kernel_size=(3, 3), stride=(1, 1), activation='relu'):
    res = Conv2D(filters, kernel_size, strides=stride, padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.0001), kernel_initializer='he_normal')(x)
    res = BatchNormalization()(res)
    res = Activation(activation)(res)
    
    res = Conv2D(filters, kernel_size, strides=stride, padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.0001), kernel_initializer='he_normal')(res)
    res = BatchNormalization()(res)
    
    shortcut = Conv2D(filters, (1, 1), strides=stride, padding='same')(x)
    shortcut = BatchNormalization()(shortcut)
    
    x = Add()([shortcut, res])
    return Activation(activation)(x)

def create_resnet_style_model(input_shape):
    inputs = Input(shape=input_shape)
    x = Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal')(inputs)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    x = MaxPooling2D((2, 2))(x)

    for filters in [64, 128, 256]:  
        x = residual_block(x, filters)
        x = MaxPooling2D((2, 2))(x)

    x = Flatten()(x)
    x = Dense(1024, kernel_initializer='he_normal')(x)
    x = ReLU()(x)
    x = Dropout(0.5)(x)  
    outputs = Dense(1, dtype='float32')(x)

    model = Model(inputs, outputs)
    model.compile(optimizer=Adam(learning_rate=5e-4), loss='mse', metrics=['mae'])  
    return model

with strategy.scope():
    input_shape = (128, 128, 3)
    model = create_resnet_style_model(input_shape)

checkpoint_cb = ModelCheckpoint(WEIGHTS_SAVE_PATH, save_best_only=True)
reduce_lr_cb = ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)
early_stopping_cb = EarlyStopping(patience=500, restore_best_weights=True)

history = model.fit(
    x_train, y_train_scaled, 
    validation_data=(x_val, y_val_scaled), 
    epochs=  ,
    batch_size=  * strategy.num_replicas_in_sync, 
    callbacks=[checkpoint_cb, reduce_lr_cb, early_stopping_cb]
)
model.load_weights(WEIGHTS_SAVE_PATH)
x_test = preprocess_data(test_paths)
test_loss, test_mae = model.evaluate(x_test, y_test_scaled)
y_pred = model.predict(x_test)

y_pred_scaled = model.predict(x_test)
y_pred = scaler.inverse_transform(y_pred_scaled)

r2 = r2_score(dadN_values_test, y_pred)
print(f"Test R^2 Score: {r2}")

epochs = range(1, len(history.history['mae']) + 1)

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.plot(epochs, history.history['mae'], label='Training MAE')
plt.plot(epochs, history.history['val_mae'], label='Validation MAE')
plt.title('Mean Absolute Error')
plt.xlabel('Epochs')
plt.ylabel('MAE')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(dadN_values_test, y_pred, 'o')
plt.plot([min(dadN_values_test), max(dadN_values_test)], 
         [min(dadN_values_test), max(dadN_values_test)], 'r--')
plt.title('R^2 Score')
plt.xlabel('Actual dadN')
plt.ylabel('Predicted dadN')
plt.text(min(dadN_values_test), max(y_pred), f'R^2: {r2:.2f}', fontsize=12, verticalalignment='top', horizontalalignment='left')

plt.tight_layout()
plt.show()
